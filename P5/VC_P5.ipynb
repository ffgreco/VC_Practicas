{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0909bac158270f",
   "metadata": {},
   "source": [
    "TAREA: Tras mostrar opciones para la detección y extracción de información de caras humanas con deepface, la tarea a entregar consiste en proponer dos escenarios de aplicación y desarrollar dos prototipos de temática libre que provoquen reacciones a partir de la información extraída del rostro. Uno de los prototipos deberá incluir el uso de algún modelo entrenado por ustedes para la extracción de información biometríca, similar al ejemplo del género planteado durante la práctica pero con diferente aplicación (emociones, raza, edad…). El otro es de temática completamente libre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2cfc1ce09ec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:27:31.254412Z",
     "start_time": "2024-11-12T22:27:17.501775Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Inizializziamo MediaPipe per il rilevamento dei volti\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)  # Usa la webcam predefinita\n",
    "\n",
    "while True:\n",
    "    # Acquisiamo un frame dalla webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertiamo il frame in RGB (richiesto da MediaPipe)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_frame)\n",
    "\n",
    "    # Controlliamo se è stato rilevato qualche volto\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            # Otteniamo la bounding box del volto\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            h, w, _ = frame.shape\n",
    "            x, y, width, height = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)\n",
    "\n",
    "            # Disegniamo un rettangolo attorno al volto\n",
    "            cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "\n",
    "            # Disegniamo i punti chiave (MediaPipe ne fornisce 6 per la face detection)\n",
    "            for keypoint in detection.location_data.relative_keypoints:\n",
    "                keypoint_x = int(keypoint.x * w)\n",
    "                keypoint_y = int(keypoint.y * h)\n",
    "                cv2.circle(frame, (keypoint_x, keypoint_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Mostriamo il frame con le rilevazioni\n",
    "    cv2.imshow(\"MediaPipe Face Detection\", frame)\n",
    "    \n",
    "    # Premere \"q\" per uscire\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Rilasciamo le risorse\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b1869",
   "metadata": {},
   "source": [
    "Inicie MediaPipe para usar el filtro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c96901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763409979.993479  188367 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1763409979.995687  199259 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763409980.000792  199259 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Inizializziamo MediaPipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=3, min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Carichiamo le immagini PNG dei filtri\n",
    "dog_ears = cv2.imread(\"assets/dog_ears.png\", -1)\n",
    "dalmatian_ears = cv2.imread(\"assets/dalmatian_ears.png\", -1)\n",
    "dog_nose = cv2.imread(\"assets/dog_nose.png\", -1)\n",
    "dog_tongue = cv2.imread(\"assets/dog_tongue.png\", -1)\n",
    "dalmatian_nose = cv2.imread(\"assets/dalmatian_nose.png\", -1)\n",
    "\n",
    "# Tipo di orecchie da usare, default dog\n",
    "ear_type = \"dog\"\n",
    "\n",
    "def overlay_image(background, overlay, x, y, w, h):\n",
    "    if x < 0:\n",
    "        overlay = overlay[:, -x:]\n",
    "        w += x\n",
    "        x = 0\n",
    "    if y < 0:\n",
    "        overlay = overlay[-y:, :]\n",
    "        h += y\n",
    "        y = 0\n",
    "    if x + w > background.shape[1]:\n",
    "        w = background.shape[1] - x\n",
    "    if y + h > background.shape[0]:\n",
    "        h = background.shape[0] - y\n",
    "    if w <= 0 or h <= 0:\n",
    "        return background\n",
    "\n",
    "    overlay = cv2.resize(overlay, (w, h))\n",
    "    alpha_overlay = overlay[:, :, 3] / 255.0\n",
    "    alpha_background = 1.0 - alpha_overlay\n",
    "\n",
    "    for c in range(3):\n",
    "        background[y:y + h, x:x + w, c] = (alpha_overlay * overlay[:, :, c] +\n",
    "                                           alpha_background * background[y:y + h, x:x + w, c])\n",
    "    return background\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "\n",
    "            left_eye = face_landmarks.landmark[33]\n",
    "            right_eye = face_landmarks.landmark[263]\n",
    "            nose = face_landmarks.landmark[1]\n",
    "            mouth_top = face_landmarks.landmark[13]\n",
    "            mouth_bottom = face_landmarks.landmark[14]\n",
    "\n",
    "            left_eye = (int(left_eye.x * w), int(left_eye.y * h))\n",
    "            right_eye = (int(right_eye.x * w), int(right_eye.y * h))\n",
    "            nose = (int(nose.x * w), int(nose.y * h))\n",
    "            mouth_top = (int(mouth_top.x * w), int(mouth_top.y * h))\n",
    "            mouth_bottom = (int(mouth_bottom.x * w), int(mouth_bottom.y * h))\n",
    "\n",
    "            eye_distance = abs(right_eye[0] - left_eye[0])\n",
    "            ear_width = int(eye_distance * 2.5)\n",
    "            nose_width = eye_distance // 2\n",
    "\n",
    "            # Selezione orecchie in base al tasto premuto\n",
    "            if ear_type == \"dog\":\n",
    "                ear_height = int(ear_width * dog_ears.shape[0] / dog_ears.shape[1])\n",
    "                frame = overlay_image(frame, dog_ears, left_eye[0] - ear_width // 4, left_eye[1] - ear_height, ear_width, ear_height)\n",
    "                nose_height = int(nose_width * dog_nose.shape[0] / dog_nose.shape[1])\n",
    "                frame = overlay_image(frame, dog_nose, nose[0] - nose_width // 2, nose[1] - nose_height // 2, nose_width, nose_height)\n",
    "            elif ear_type == \"dalmatian\":\n",
    "                ear_height = int(ear_width * dalmatian_ears.shape[0] / dalmatian_ears.shape[1])\n",
    "                frame = overlay_image(frame, dalmatian_ears, left_eye[0] - ear_width // 4, left_eye[1] - ear_height, ear_width, ear_height)\n",
    "                nose_height = int(nose_width * dalmatian_nose.shape[0] / dalmatian_nose.shape[1])\n",
    "                frame = overlay_image(frame, dalmatian_nose, nose[0] - nose_width // 2, nose[1] - nose_height // 2, nose_width, nose_height)\n",
    "\n",
    "            # Bocca aperta: aggiungi lingua\n",
    "            mouth_opening_height = abs(mouth_bottom[1] - mouth_top[1])\n",
    "            mouth_open_threshold = h / 20\n",
    "            if mouth_opening_height > mouth_open_threshold:\n",
    "                tongue_width = nose_width * 2\n",
    "                tongue_height = int(tongue_width * dog_tongue.shape[0] / dog_tongue.shape[1])\n",
    "                frame = overlay_image(frame, dog_tongue, nose[0] - tongue_width // 2, mouth_bottom[1], tongue_width, tongue_height * 2)\n",
    "\n",
    "    # Aggiungiamo legenda in alto a sinistra\n",
    "    legenda = [\n",
    "        \"Premi 1: orecchie cane\",\n",
    "        \"Premi 2: orecchie dalmata\",\n",
    "        \"Premi q: esci\"\n",
    "    ]\n",
    "    y0, dy = 30, 30\n",
    "    for i, line in enumerate(legenda):\n",
    "        y = y0 + i * dy\n",
    "        cv2.putText(frame, line, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Filtro Snapchat\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    elif key == ord(\"1\"):\n",
    "        ear_type = \"dog\"\n",
    "    elif key == ord(\"2\"):\n",
    "        ear_type = \"dalmatian\"\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
